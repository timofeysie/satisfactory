# May

## Add json file name to the exported articles json

How does that filename get generated?

Where are the functions that do it?

generateList() { }

saveFile(articles: any) {
const jsonString = JSON.stringify(articles);
fs.writeFile('./articles/articles.json', jsonString, (err) => { ... }

The written file is here:

articles\articles.json

Is it OK to use that file directly from different projects on this laptop? Previously I copy it over to Tundra, but now that the AMP frontend is being worked on, it needs to be copied there as well.

## Summaries

Error: ENOENT: no such file or directory, open 'C:\Users\timof\repos\satisfactory\apps\nest-demo\src\app\bart\summaries\https%3A%2F%2Fwww.sportingnews.com%2Fau%2Fsoccer%2Fnews%2Fchelsea-vs-liverpool-score-live-fa-cup-final%2Fiurgrbkbcgva0algwhqhpyeo.txt'

This is becoming a pain in the ass. My first suspicion is that the target article is too long for goose. How do we test that out? Does this take precendence over the important story creation?

It's worth looking into. Not sure which has priority.

## Create a story from the dog-demo template upon article post

This is where the post gets written.

apps\nest-demo\src\app\trends\trends.service.ts

Since this is not the ideal way to generate stories from json files, it's best to not spend too much time on it.

Still, we don't want to just have a long html string literal there to plug in our variables to.

However, creating a lib to be used by a service didn't seem to work when we were trying to collect bing search results to put together for content to train our NPL model to create content with.

I think it's worth using a provider.

### NestJS providers

<https://docs.nestjs.com/providers>

A service is an injected provider used by the controller. Maybe then we just need a new service to handle this exclusively.

apps\nest-demo\src\app\trends\trends.controller.ts

apps\nest-demo\src\app\trends\story.service.ts

Something about this seems a bit funny:

```ts
  @Post()
  create(@Body() createTrendDto: any) {
    this.storyService.create(createTrendDto);
    return this.trendsService.create(createTrendDto);
  }
```

We don't really use the result from either of these yet, so I suppose it's all right. The user can optimistically look for the generated results elsewhere in the app. Since some of the async tasks are long running sometimes its not possible to keep the connection open.

Or maybe it is. Maybe we want to do something like Promises.all here and send aggrigated results back. But for now, let's just focus on the functionality of generating a story from a template.

Renaming:
posts\-UFC-lightweight-champion-Charles-Oliveira-on-a-career-low.json

To:
posts\UFC-lightweight-champion-Charles-Oliveira-on-a-career-low.json

When trying to test the story.service, this laptop crashed. There were two images donwloaded, and the usual summaries scraped and barted. No summaries were created, so defo the barting needs to be looked at.

I think we gotta stop the automatic ml work. Let the user kick off each step as they go. Some images will not need to be used to generate other styled images. An article with images from the public domain might be wanted to support the info in the stories.

So the only automatic generation happens when the user goes from the first post form to the final page. The images selected in the previous stage are downloaded and then generation starts, at the same time the selected articles from the first step are downloaded and summarized. This is causing the laptop to crash sometimes. Not good.

I think the best thing to do is either not to have any images in the test-images directory, or don't choose any articles to summarize.

Or just keep all the directories empty and just focus on one action at a time for now.

## The download article feature

Hit the download from cloud icon on a news item and this is what we see:

bart.controller.getArticleSummary: article {
link: 'https://www.usatoday.com/story/news/nation/2022/05/14/buffalo-new-york-shooting-tops/9778322002/'
}
bart.service.getArticleSummary: articleUrl <https://www.usatoday.com/story/news/nation/2022/05/14/buffalo-new-york-shooting-tops/9778322002/>

No file is written.

Hit that again and see this:

bart.controller.getArticleSummary: article {
link: 'https://www.nytimes.com/live/2022/05/14/nyregion/buffalo-shooting'
}
bart.service.getArticleSummary: articleUrl <https://www.nytimes.com/live/2022/05/14/nyregion/buffalo-shooting>
bart.service getArticleSummary service err No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (<https://huggingface.co/sshleifer/distilbart-cnn-12-6>)
bart.service getArticleSummary service err No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (<https://huggingface.co/sshleifer/distilbart-cnn-12-6>)

That message is an indication that a file is written:

service err No model was supplied, defaulted to sshleifer

## Image content for stories

I need a sample like the dog-demo story using a sample post.

Downloading the dog-demo images seems to be the only way to get the correct dimensions. The google inspector lies about the size of the image for some reason.

Downloading the square image, it uses a .webp format?

story_dog2_square.webp

And guess what? It's a Google thing: _WebP is a modern image format that provides superior lossless and lossy compression for images on the web. Using WebP, webmasters and web developers can create smaller, richer images that make the web faster. 22 Mar 2022_

That looks pretty recent. Wikipedia says not so fast: _Google announced the WebP format in September 2010, and released the first version of its supporting library in April 2018._

Finding the size of the four images used for the poster and the first page of the dog-demo story proved a bit of a challenge. I had to download the image, and then, there are three different views of a file property in the new Windows which installed itself automatically on my laptop last week. This was despite me never authorizing that and delaying it each time it asked me. All of a sudden, one morning, the installation had happened. I think it was a disk space issue, as the hard drive was getting full, so I must have deleted enough content for the download to happen and complete.

```txt
story_dog2_portrait.jpg  720 x 960
story_dog2_square.jpg 720 x 720
story_dog2_landscape.jpg 720 x 541
story_dog2.jpg 720 x 1279
```

Now we need our first story to create. What's it going to be? I feel like we actually want a different route for creating stories. Currently, we have a story service that is used alongside the trends service. It makes sense when you do a post, but that means I can only make one from a new post.

It makes sense to just create a new route just to use this tool: [ImageMagic](https://stackabuse.com/working-with-images-in-node-js-graphicsmagick-and-imagemagick/).

How do we add new route again? MECO BECO?

```txt
nx generate @nestjs/schematics:resource image --sourceRoot apps/nest-demo/src/app
```

The output looks like this:

```shell
-demo/src/appimof\repos\satisfactory>
√ What transport layer do you use? · rest
√ Would you like to generate CRUD entry points? (Y/n) · true
"SchematicsNestResource" schema is using the keyword "id" which its support is deprecated. Use "$id" for schema ID.
CREATE apps/nest-demo/src/app/image/image.controller.spec.ts
CREATE apps/nest-demo/src/app/image/image.controller.ts
CREATE apps/nest-demo/src/app/image/image.module.ts
CREATE apps/nest-demo/src/app/image/image.service.spec.ts
CREATE apps/nest-demo/src/app/image/image.service.ts
CREATE apps/nest-demo/src/app/image/dto/create-image.dto.ts
CREATE apps/nest-demo/src/app/image/dto/update-image.dto.ts
CREATE apps/nest-demo/src/app/image/entities/image.entity.ts
UPDATE apps/nest-demo/src/app/app.module.ts
```

I guess actually we will want to use GraphicsMagic: _GraphicsMagick has over ImageMagick include more efficiency, a smaller size, fewer security exploits and is generally more stable than ImageMagick._

Sorry, that spelling looks a little odd: Magick. Apparently, it's an archaic spelling of magic.

I'm not sure which version to download. This is an old school app, served via Sourceforge.

The latest version is 1.3.38 (March 26, 2022)

The maintainer goes on a rant on main page of the files under the Special Issues title:

_The FTP site ftp.graphicsmagick.org is now shut down due to a lack of bandwith, extremely abusive users (including from Google and customers of Amazon Web Services), and a lack of support from the user community. Another factor is that FTP support has been removed from popular web browsers. This is very unfortunate since the site served multiple usages, including providing a lot of historical data (e.g. related to PNG) which may not be available elsewhere._

_GraphicsMagick really does need some additional productive volunteers. For several years now, the burden has entirely been on me (Bob Friesenhahn). I have been sheparding the project for 20 years already (and contributed to ImageMagick and GraphicsMagick combined for 26 years already). It is not reasonable to expect someone with a full time job (and expecting to retire in a few years) to do all of the work._

GraphicsMagick-1.3.38-windows.7z

For GraphicsMagick:

$ npm install gm

Next, the docs of course are not TypeScript. The ol'JS style is:

const gm = require('gm');

The TypeScript was to do this is:

```js
import * as gm from 'gmi';
```

Put that in apps/nest-demo/src/app/image/image.service.ts

Then we get this error:

Cannot find module 'gmi' or its corresponding type declarations.ts(2307)

One way around this is to create our own generic types definition. Create a file in the same directory like this:

gmi.d.ts

And put this in it:

```ts
declare module 'gmi' {
  let gmi: any;
  export = gmi;
}
```

Next, let's use it on a route:

```ts
  findOne(imagePath: string) {
    GM(imagePath).identify(function (err, value) {
      console.log(value);
      if (err) {
        console.log(err);
      }
    });
    return `This action returns a #${imagePath} image`;
  }
```

What is a path to on of our images? Will it work with an s3 bucket url? Worth a try.

But trying to start the server fails:

```shell
ERROR in ./apps/nest-demo/src/app/image/image.service.ts
Module not found: Error: Can't resolve 'gmi' in 'C:\Users\timof\repos\satisfactory\apps\nest-demo\src\app\image'
There was an error with the build. See above.
C:\Users\timof\repos\satisfactory\dist\apps\nest-demo\main.js was not restarted.
Error from chokidar (C:\): Error: EBUSY: resource busy or locked, lstat 'C:\DumpStack.log.tmp'
```

The package installed is:

    "gm": "^1.23.1",

So I think we have to change that import to gm and not gmi.

Then the server runs. Now to test that route.

```sh[Nest] 9244   - 20/05/2022, 10:26:47 am   [RoutesResolver] ImageController {/api/image}: +2ms
[Nest] 9244   - 20/05/2022, 10:26:47 am   [RouterExplorer] Mapped {/api/image, POST} route +1ms
[Nest] 9244   - 20/05/2022, 10:26:47 am   [RouterExplorer] Mapped {/api/image, GET} route +2ms
[Nest] 9244   - 20/05/2022, 10:26:47 am   [RouterExplorer] Mapped {/api/image/:id, GET} route +2ms
[Nest] 9244   - 20/05/2022, 10:26:47 am   [RouterExplorer] Mapped {/api/image/:id, PATCH} route +2ms
[Nest] 9244   - 20/05/2022, 10:26:47 am   [RouterExplorer] Mapped {/api/image/:id, DELETE} route +1ms
```

<http://localhost:3333/api/image>

Works.

<http://localhost:3333/api/image/https://one-public-bucket.s3.ap-southeast-2.amazonaws.com/AI+image+for+Man+City+vs+Atl%C3%A9tico+Madrid_Hayao.jpg>

Dones't work:

{"statusCode":404,"message":"Cannot GET /api/image/<https://one-public-bucket.s3.ap-southeast-2.amazonaws.com/AI+image+for+Man+City+vs+Atl%C3%A9tico+Madrid_Hayao.jpg>","error":"Not Found"}

Just using any filename will result in this error:

```err
TypeError: gm is not a function
    at ImageService.findOne (C:\Users\timof\repos\satisfactory\dist\apps\nest-demo\webpack:\apps\nest-demo\src\app\image\image.service.ts:18:5)
```

This import seems to work when there are two names:

```ts
import * as gmi from 'gm';
```

apps\toonify\src\data\output-images\taj_mahal_width_500_model_mosaic_4e5_e2.jpg

Error: ENOENT: no such file or directory, open 'C:\Users\timof\repos\satisfactory\dist\apps\nest-demo\apps\toonify\src\data\output-images\taj_mahal_width_500_model_mosaic_4e5_e2.jpg'

Why is it looking in the dist folder?

What if I put the file here:

C:\Users\timof\repos\satisfactory\dist\apps\nest-demo\assets\AI-image-for-Man-City-vs-Atlético-Madrid.json

no such file or directory, open
C:\Users\timof\repos\satisfactory\dist\apps\nest-demo\taj_mahal_width_500_model_mosaic_4e5_e2.jpg'

Or here:

C:\Users\timof\repos\satisfactory\apps\nest-demo\taj_mahal_width_500_model_mosaic_4e5_e2.jpg

Error: ENOENT: no such file or directory, open C:\Users\timof\repos\satisfactory\dist\apps\nest-demo\taj_mahal_width_500_model_mosaic_4e5_e2.jpg

Or here rather:

dist\apps\nest-demo\taj_mahal_width_500_model_mosaic_4e5_e2.jpg

ImageService.findOne: imagePath apps/toonify/src/data/output-images/taj_mahal_width_500_model_mosaic_4e5_e2.jpg
[Nest] 4180 - 20/05/2022, 1:22:45 pm [ExceptionsHandler] ENOENT: no such file or directory, open 'C:\Users\timof\repos\satisfactory\
dist\apps\nest-demo\taj_mahal_width_500_model_mosaic_4e5_e2.jpg' +32587ms
Error: ENOENT: no such file or directory, open 'C:\Users\timof\repos\satisfactory\
dist\apps\nest-demo\taj_mahal_width_500_model_mosaic_4e5_e2.jpg'

Copy the file there and use that path like this:

```ts
  findOne(imageName: string) {
    const imagePath = 'dist/apps/nest-demo/'+imageName;
    gmi(imagePath).identify(function (err, value) {
```

But it's a "TypeError: gmi is not a function"

Well, is that a problem in node or the usage in TypeScript.

I don't think it's usage. We use fs the same way:

```ts
import * as fs from 'fs';
export class BartService {
  loadSummary() {
    const path = `./apps/nest-demo/src/app/bart/summary.txt`;
    fs.readFile(path, 'utf-8', (err, file) => {
      ...
```

Well, it;s a little different.

Here are what [the official docs](https://www.npmjs.com/package/gm) say:

Use ImageMagick instead of gm
Subclass gm to enable ImageMagick

```js
var fs = require('fs')
  , gm = require('gm').subClass({imageMagick: true});
// resize and remove EXIF profile data
gm('/path/to/my/img.jpg')
.resize(240, 240)
...
```

What is subClass?

How long have I been using JavaScript?

Full time since 2015. And I've never seen that. Too long spent in Angular and React lands. I grew up with import, the older generation grew up with require.

<http://localhost:3333/api/image/taj_mahal_width_500_model_mosaic_4e5_e2.jpg>

I added a comment to [this open issue](https://github.com/aheckmann/gm/pull/827) from 2021 (last year) and I think it's time to try another lib at this point.

## Javascript image resizing libs

- [Sharp](https://www.positronx.io/node-rotate-resize-image-size-using-sharp-module-tutorial/)

Sharp works to create webp format also.

### Sharp

npm install sharp

```js
const sharp = require('sharp');
sharp('./assets/kid.jpg')
  .rotate()
  .resize(400)
  .jpeg({ mozjpeg: true })
  .toFile('./assets/img/new_resized_kid.jpg', (err, info) => {
    console.log('File has successfully resized.');
  });
```

Same thing as gm:

TypeError: sharp is not a function
at ImageService.findOne (C:\Users\timof\repos\satisfactory\dist\apps\nest-demo\webpack:\apps\nest-demo\src\app\image\image.service.ts:20:12)

[This SO](https://github.com/RocketChat/Rocket.Chat/issues/13847) is quite an involved solution:

_this really does happen on arm64. the problem is that the rocketchat tarball contains bundle/programs/server/npm/node_modules/sharp/vendor/lib/ filled with a bunch of libraries for x86. when npm tries to install, it gets errors on those and sharp ends up not being configured correctly. (i suspect this happens to any non-x86 architecture or perhaps any linux distro that has a divergent libc version)_

_if that vendor dir is removed prior to building, sharp will go and fetch a new libvips tarball (the lib it has trouble linking) and recompile everything it needs._

This is not an arm64 issue, because this laptop has an intel Core6 chip. But maybe npm is not doing the right thing during installation.

It's also a little too suspicious now that both libraries would be unworkable in TypeScript, so I was encouraged to give other formats a try:

```ts
import sharp from 'sharp';
```

ReferenceError: sharp is not defined

Actually, this does work with a capital S for import Sharp.

Give GraphicsMagick a try again, and this is the output on the server:

value {}
err Error: Could not execute GraphicsMagick/ImageMagick: gm "identify" "-ping" "-verbose" "dist/apps/nest-demo/taj_mahal_width_500_model_mosaic_4e5_e2.jpg" this most likely means the gm/convert binaries can't be found

npm install -g gm

However, I still see the same error. And considering that the app with eventually be deployed, Sharp is working via npm only, without a requirement for the cli, so using Sharp will make the app easier to deploy.

Here is our first image meta data:

```js
metadata {
  format: 'jpeg',
  width: 400,
  height: 278,
  space: 'srgb',
  channels: 3,
  depth: 'uchar',
  density: 72,
  chromaSubsampling: '4:2:0',
  isProgressive: true,
  hasProfile: false,
  hasAlpha: false
}
```

Next, how to crop an image into landscape, portrait and square formats.

### Extract/crop a region of the image

[The docs](https://sharp.pixelplumbing.com/api-resize) shows the API like this:

```txt
Use extract before resize for pre-resize extraction.
Use extract after resize for post-resize extraction.
Use extract before and after for both.
```

They show parameters, and have examples.

```js
sharp(input)
  .extract({
    left: leftOffsetPre,
    top: topOffsetPre,
    width: widthPre,
    height: heightPre,
  })
  .resize(width, height)
  .extract({
    left: leftOffsetPost,
    top: topOffsetPost,
    width: widthPost,
    height: heightPost,
  })
  .toFile(output, function (err) {
    // Extract a region, resize, then extract from the resized image
  });
```

Actually, we wouldn't need that second step. This works for our purpose:

```ts
  findOne(imageName: string) {
    const imagePath = 'dist/apps/nest-demo/'+imageName;
    const saveImagePath = 'dist/apps/nest-demo/saved' + imageName;
    console.log('imagePath', imagePath);
    const image = Sharp(imagePath);
    image
      .metadata()
      .then((metadata) => {
        console.log('meta', metadata);
        Sharp(imagePath)
          .extract({
            left: 50,
            top: 50,
            width: metadata.width - 100,
            height: metadata.height - 100,
          })
          .toFile(saveImagePath, (err) => {
            console.log('done');
            if (err) {
              console.log('err', err);
            }
            // Extract a region of the input image, saving in the same format.
          });
      });
  }
```

## Creating landscape, portrait and square versions of an image

Next, we want to use this api to generate all the images we need. For now, we just want to create the aspects we need with sane defaults.

For example, the offsets should place the cropped result as coming from the middle of the image.

Also, we want to put the image type in between the end of the image name and the file extension.

If the calling code knows the file name without the extension, and the extension, then they can pass it in. We may as well let the caller decided what the offset type of image needed are. Default will be all types and centered. I imagine we will want the frontend to allow the user to move the cropped box around to center on what they want, but that feature can come later.

So with this input:

dist\apps\nest-demo\taj_mahal_width_500_model_mosaic_4e5_e2.jpg

We would want to see these files generated:

```txt
dist\apps\nest-demo\taj_mahal_width_500_model_mosaic_4e5_e2-landscape.jpg
dist\apps\nest-demo\taj_mahal_width_500_model_mosaic_4e5_e2-portrait.jpg
dist\apps\nest-demo\taj_mahal_width_500_model_mosaic_4e5_e2-square.jpg
dist\apps\nest-demo\taj_mahal_width_500_model_mosaic_4e5_e2-thumbnail.jpg
```

The function we want I think is this:

### Resize image to width, height or width x height

When both a width and height are provided, the possible methods by which the image should fit these are:

```txt
cover: (default) Preserving aspect ratio, ensure the image covers both provided dimensions by cropping/clipping to fit.
contain: Preserving aspect ratio, contain within both provided dimensions using "letterboxing" where necessary.
inside: Preserving aspect ratio, resize the image to be as large as possible while ensuring its dimensions are less than or equal to both those specified.
outside: Preserving aspect ratio, resize the image to be as small as possible while ensuring its dimensions are greater than or equal to both those specified.
```

Now we need a real image to perform that on. Load up a test_image.

Kylian_Mbapp%C3%A9_2018.jpg

C:\Users\timof\repos\satisfactory\apps\toonify\src\test_img\Kylian_Mbapp%C3%A9_2018.jpg

const dir = 'apps/toonify/src/test_img/';

Dimensions from the download folder: 535 x 921.

It's a long skyscraper image. Now what are our target dimensions again?

```txt
portrait  720 x 960
square 720 x 720
landscape 720 x 541
regular 720 x 1279
```

_Make sure that the image linked to your <amp-story> poster-portrait-src attribute is at least 640x853px and use an aspect ratio of 3:4._

Error: Input file is missing:
apps/toonify/src/test_img/Kylian_Mbappé_2018.jpg
apps\toonify\src\test_img\Kylian_Mbapp%C3%A9_2018.jpg

Our image is 535 x 921.
We need 640 x 853.

It's close. Should we be increasing the size to get to that?

### How to calculate aspect ratio?

_Divide the original height by the original width and then multiply this number by the new width to get the new height._

What we want for portrait 720 x 960.
The original image is: 535 x 921.

535 / 921 = 0.5808903365906623

0.5808903365906623 X 640 = 372.

So the new image should be 372 X 640.

```ts
const originalRatio = metadata.width / metadata.height;
const newHeight = 640;
const newWidth = Math.round(newHeight * originalRatio);
Sharp(imagePath).extract({
  left: 50,
  top: 1,
  width: newWidth,
  height: newHeight,
});
```

square 720 x 720
landscape 720 x 541

We're are definitely going to need to pass in the offset. Using a default fails on tis first go.

We want to focus on a persons face, or an object in the picture, a default will never work for that.

ML can be used to spot people's faces, but the trends are not always about people.

This means the beginning of a UI to control the crop square. We could just show the default cropped image with arrows and a text input for quick choices. Show the original dimensions, and let the user either move the square around, or enter a new legit offset.

We don't want to be reproducing Gimp or Photoshop here. Just the basics.

Either way, we need to pass some date from the front end, such as image name and x/y offsets.

That means we need to post the data. The get one can be just to get the meta data which would be needed for non-story poster images.

Then, we will need to display all four images and let the user edit them one at a time.

It's kind of a epic feature. Do we need another document?

When the background removal function is working, the images will need to be moved to place a fore ground image on a background. So a generic UI that lets the user drag around a specific aspect container in an image.

This way it can be used to select poster images, and to add a foreground image to a background. It's basically just about setting up the canvas, and then drawing a square, or another image with a transparent background.

So year, it's actually a huge epic. There are multiple components needed on the frontend, as well as multiple APIs from the backend.

Steps to do:

1. the GET returns the meta data
2. the POST takes a filename and offsets and saves a cropped image
3. the frontend needs a new component/modal or whatever and a way to select the type
4. move the select container around
5. upload the images to the S3 bucket

Something like that.

### #3 a new container for images

What is the model where an async service is returned?

What is the route we're using to get a reply from a python script and the frontend waits for it?

It's the GPT text generation route. Which one is that?

'http://localhost:3333/api/generate/' + encodedSeed;

The consumer then for the endpoint will be to show the data next to the original image:

libs\trends\src\lib\components\post-creation-form\post-creation-form.component.html

I guess we can call the component something like image preview.

But really, that should be a container. The logic to center the various posters can be contained there. It can even post it's own pictures to the S3 bucket. Also, we might want to use the image edit feature outside of the post creation form.

The container would look something like this pseudo code:

```pseudo
<image-preview>
  <center-image>
    <meta-img>
    <select>
  <image-controls>
    <upload>
```

OK, so what's the scaffolding command for these?

nx generate @nrwl/angular:component containers/image-preview --project=trends

nx generate @nrwl/angular:component components/crop-image --project=trends

#### Waiting for the file name

Trying to use the [async pipe](https://stackoverflow.com/questions/41389124/angular-2-how-to-make-child-component-wait-for-async-data-to-be-ready) and a behavior subject to wait for the image file to be chosen before calling the service to get the meta data for the file.

```txt
core.js:6456 ERROR Error: InvalidPipeArgument: 'Kylian_Mbapp%C3%A9_2018.jpg' for pipe 'AsyncPipe'
    at invalidPipeArgumentError (common.js:4260:1)
    at AsyncPipe._selectStrategy (common.js:4362:
```

I'm not sure we need the async pipe. Yep, it works without the async pipe, and when the container gets an image name passed in, and using a setter (as shown in the SO link above) for the BehaviorSubject we can check when the name arrives.

### The file path

GET APIs are a bit limited in being able to pass info to the server. We have to basically fix the directory that the image can be loaded from. The problem is, we want a more generic version.

Yes, we have the meta data for a generated image now:

Request URL: <http://localhost:3333/api/image/Kylian_Mbapp%C3%A9_2018_Paprika.jpg>

```js
meta {
  format: 'jpeg',
  width: 264,
  height: 452,
  space: 'srgb',
  channels: 3,
  depth: 'uchar',
  density: 72,
  chromaSubsampling: '4:2:0',
  isProgressive: false,
  hasProfile: false,
  hasAlpha: false
}
```

The original image size is 535 x 921.

Since actually size is important for the posters, I think we want to first crop an image in the test_image directory, and then generate the images from those which will all be slightly different, and offer the ability to use different model for the same image. Goody goody.

So then switch the fixed path back to test_img

The meta data for the original image then is:

```js
meta {
  format: 'jpeg',
  width: 535,
  height: 921,
  space: 'srgb',
  channels: 3,
  depth: 'uchar',
  density: 300,
  chromaSubsampling: '4:2:0',
  isProgressive: false,
  resolutionUnit: 'inch',
  hasProfile: false,
  hasAlpha: false,
  orientation: 1,
  exif: <Buffer 45 78 69 66 00 00 4d 4d 00 2a 00 00 ...
```

That's a big drop in the density value from 300 to 72. Now can someone explain what image density is? Is that the same thing as pixel density or resolution?

_Note that pixel density is not the same as resolution, where the former describes the amount of detail on a physical surface or device, the latter describes the amount of pixel information regardless of its scale. Considered in another way, a pixel has no inherent size or unit (a pixel is actually a sample), but when it is printed, displayed, or scanned, then the pixel has both a physical size (dimension) and a pixel density (ppi)._

Great. Thanks for that.

The original has three extra fields:

```txt
resolutionUnit: 'inch',
orientation: 1,
exif: <Buffer 45 78 69 66 ...
```

### What is the gan/bucket directory for?

From the May 25th commit:

Untracked files:
(use "git add <file>..." to include in what will be committed)
apps/nest-demo/src/app/gan/bucket/

### Buttons

Now we have two buttons, "Choose source image" and "Choose generated image" which call the same function: this.fileInput.nativeElement.click();

That just opens the dialog. The difference is in which path will be used to get the meta data from. The problem is, the user just selects a file, and then we don't know which method was used. I hate to say it, but a boolean flag could solve this.

Whenever I start to reach for boolean flags, there feels like a problem with the architecture. The flags complicate the class, and are hard to figure when looking at code later. At least it should be named very carefully.

useImageSourcePath? That's OK?

```ts
/** This is passed into the <demo-app-image-preview> component so that we know
 * this is a source image, not the generated image.
 */
useImageSourcePath: boolean;
```

The problem comes from a fixed API. We can only pass a file name in, not a path.

Generally, I think the <demo-app-image-preview> component will only be used to work with source images. Given that so much is lost in detail and size when the images are generated from the source. So maybe we don't really need this. It's OK to fix it. The user might see the semantic buttons as used for different purposes.

Currently, when the user chooses a generated image with that button, we don't show that generated image, just the file name. That's because it's only determining that the image needs to be uploaded to an S3 bucket. We don't show that image. I suppose we could. Now that we have a button to work with source images, it might be needed to show the difference in the functionality being used.

And, we probably want a manual way to upload multiple images to S3. Or, just have that happen on AMP story creation. I'm not sure yet if they need to be used for an AMP page. So I supposed we can go with the automatic version for now, and we don't need a flag after all.

### Displaying the meta data

This is my first time using a mat-table, and it shows:

core.js:6456 ERROR Error: Missing definitions for header, footer, and row; cannot determine which columns should be rendered.

What if I don't what headers? It seems like this table might be an overkill.

There will be four rows though.

Aspect height width
Original yyy xxx
Landscape --- ---
Portrait --- ---
Square --- ---

We don't know the values until we do the calculations for the different aspects. So I think the table is not a good idea. How about a grid? The user probably doesn't care about the height/width, but showing those will help during development for sure.

### Calculating the aspect dimensions

What should the payload for the post look like?

```js
{
  path: "",
  fileName: "xxx",
  original: { width: 535, height: 921 },
  portrait: { left: leftOffsetPre, top: topOffsetPre, width: widthPre, height: heightPre },
  landscape: { left: leftOffsetPre, top: topOffsetPre, width: widthPre, height: heightPre },
  square: {left: leftOffsetPre, top: topOffsetPre, width: widthPre, height: heightPre },
}
```

The user just needs to supply the top, left and length of the crop.  How they do this is still undecided. It would be nice to show four original images with little black rectangles in each showing the crop and let the user drag and resize those.  That would also take a lot of work.  Initially at least, I think we just show the three default cropped images using a best guess (we could focus on faces using AI) and let the user input values and update the image.  A lot less work.

From the notes above:

```txt
portrait  720 x 960
square 720 x 720
landscape 720 x 541
regular 720 x 1279
```

_Make sure that the image linked to your <amp-story> poster-portrait-src attribute is at least 640x853px and use an aspect ratio of 3:4._

### Separate POST for each aspect

It's a better idea to have the POST endpoint only work on one aspect at a time.  This way, the user can update one aspect at a time, or the front end can loop through all three and generate the default images at the start when an original image is created.  So then our payload should look like this:

```js
{
  path: "",
  fileName: "xxx",
  original: { width: 535, height: 921 },
  aspect: "portrait",
  new: { left: leftOffsetPre, top: topOffsetPre, width: widthPre, height: heightPre },
}
```

And we need a new input as reusing the current file chooser tries to upload a non-existent generated image.

Had to also add a new emitter to set the chosen original image without performing any of the stuff that the selected generated image does.

@Output() originalImageSelected = new EventEmitter<string>();

After all this, the file is not found.

What we are looking for:

apps\toonify\src\test_img\Kylian_Mbapp%C3%A9_2018.jpg

What node is saying:

imagePath apps/toonify/src/test_img/Kylian_Mbapp%25C3%25A9_2018.jpg
(node:19188) UnhandledPromiseRejectionWarning: Error: Input file is missing:
apps/toonify/src/test_img/Kylian_Mbapp%25C3%25A9_2018.jpg

See it?  The encodeURI is doing that.

The next drama:

(node:21772) UnhandledPromiseRejectionWarning: TypeError: Cannot read property 'width' of undefined
    at C:\Users\timof\repos\satisfactory\dist\apps\nest-demo\webpack:\apps\nest-demo\src\app\image\image.service.ts:37:31
(Use `node --trace-warnings ...` to show where the warning was created)

### TODO

- (done) add service to call the image GET endpoint
- (done) add input button to load image
- (done) add the h/w to the json for the image
- (done) calculate the default sizes for the three aspects
- (done) create a POST endpoint to take image name, type and desired offset
- allow the user to change the offset
